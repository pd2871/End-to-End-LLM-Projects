{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWuizIz-rWcN",
        "outputId": "d9a23b08-a4cb-4ef5-8273-6739d97cc3fe"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers==4.37.2\n",
        "!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUgq7rMMrvRI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsAGRcVNsAeD"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVFFCfPgsExz"
      },
      "outputs": [],
      "source": [
        "model_id = \"llava-hf/llava-1.5-7b-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "39edb5ab95f74f3e961bce9c8d763342",
            "d8d2dedc3792421db386d1cb5d603374",
            "ffe339cce05546c384487b232b8137bd",
            "f9bb704cbea64f95a2d4808975cc71ee",
            "faa3cf9476d945ddb04a9c149825831b",
            "607f0633ebcf40e4aa0b3acbd3b022a8",
            "c32d3736f87f495f944b37d35c8884ed",
            "fc807b69b0aa47d9926eaa1adff16897",
            "eafb55cca4c94e899b05677e3b3b5b29",
            "a596e5e4c2514864a76d9f6540b893a2",
            "cba69ea4f8604a939f71aab783cc989d"
          ]
        },
        "id": "lBe0yQpGsHwY",
        "outputId": "5ce8fe3e-2587-4175-86a4-e478d0e7a9f4"
      },
      "outputs": [],
      "source": [
        "# pipe = pipeline(\"image-to-text\",\n",
        "#                 model=model_id,\n",
        "#                 model_kwargs={\"quantization_config\": quantization_config})\n",
        "\n",
        "pipe = pipeline(\"text-to-text\",\n",
        "                model=model_id,\n",
        "                model_kwargs={\"quantization_config\": quantization_config})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SviuyUksLC3"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPSfugatEBo"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "wmuSx4BHtMVx",
        "outputId": "29f9571f-a4eb-44a9-e7c7-07b459191d52"
      },
      "outputs": [],
      "source": [
        "# image_path = \"img.jpg\"\n",
        "# image = Image.open((image_path))\n",
        "# image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbpnkeg9tjU2",
        "outputId": "b218b2d3-0a87-4b36-9454-1558b58b3ebe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y38zRFArtnsw",
        "outputId": "64f4b95c-e563-4163-8692-9cc899b65dda"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "print(locale.getlocale())  # Before running the pipeline\n",
        "# Run the pipeline\n",
        "print(locale.getlocale())  # After running the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe(model, audio):\n",
        "\n",
        "    # Check if the audio input is None or empty\n",
        "    if audio is None or audio == '':\n",
        "        return ('','',None)  # Return empty strings and None audio file\n",
        "\n",
        "    # language = 'en'\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "\n",
        "    return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_speech(text, file_path):\n",
        "    language = 'en'\n",
        "\n",
        "    audioobj = gTTS(text = text,\n",
        "                    lang = language,\n",
        "                    slow = False)\n",
        "\n",
        "    audioobj.save(file_path)\n",
        "\n",
        "    return file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQHSEW4Xtsbh",
        "outputId": "4faa5158-cdad-437b-9441-b8af63d219a3"
      },
      "outputs": [],
      "source": [
        "max_new_tokens = 200\n",
        "\n",
        "# prompt_instructions = \"\"\"\n",
        "# Describe the image using as much detail as possible,\n",
        "# is it a painting, a photograph, what colors are predominant,\n",
        "# what is the image about?\n",
        "# \"\"\"\n",
        "\n",
        "text = 'How are you doing?'\n",
        "\n",
        "\n",
        "prompt_instructions = \"\"\"\n",
        "You are a voice assistant bot, give the brief answer based on the queries from the users.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(text, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "# outputs\n",
        "# print(outputs[0][\"generated_text\"])\n",
        "for sent in sent_tokenize(outputs[0][\"generated_text\"]):\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mpzX2vuuMuu"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_1LYQMFuPy2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from gtts import gTTS\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIQBSuSEuTaF",
        "outputId": "6eeb11c7-d91a-4396-a69b-f4ef1388183e"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lArpvJ4quVA3",
        "outputId": "5eb2f8f2-442e-4580-cac0-d7fb1c3413e2"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"medium\", device=DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNAZ3ukZuxjz"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eubq1iX1ug_p",
        "outputId": "c4ee34db-a96f-4586-8261-e538d205cf86"
      },
      "outputs": [],
      "source": [
        "input_text = 'What color is the microphone in image?'\n",
        "input_image = 'img.jpg'\n",
        "\n",
        "# load the image\n",
        "image = Image.open(input_image)\n",
        "\n",
        "# prompt_instructions = \"\"\"\n",
        "# Describe the image using as much detail as possible, is it a painting, a photograph, what colors are predominant, what is the image about?\n",
        "# \"\"\"\n",
        "\n",
        "# print(input_text)\n",
        "prompt_instructions = \"\"\"\n",
        "Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n",
        "\"\"\" + input_text\n",
        "prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "# print(prompt)\n",
        "\n",
        "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "\n",
        "match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
        "\n",
        "if match:\n",
        "    # Extract the text after \"ASSISTANT:\"\n",
        "    extracted_text = match.group(1)\n",
        "    print(extracted_text)\n",
        "else:\n",
        "    print(\"No match found.\")\n",
        "\n",
        "for sent in sent_tokenize(outputs[0][\"generated_text\"]):\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8uvFDn8u_IP"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yixcargNu_yM"
      },
      "outputs": [],
      "source": [
        "## Logger file\n",
        "tstamp = datetime.datetime.now()\n",
        "tstamp = str(tstamp).replace(' ','_')\n",
        "logfile = f'{tstamp}_log.txt'\n",
        "def writehistory(text):\n",
        "    with open(logfile, 'a', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "        f.write('\\n')\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0CZcEqdvDV9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "def img2txt(input_text, input_image):\n",
        "\n",
        "    # load the image\n",
        "    image = Image.open(input_image)\n",
        "\n",
        "    writehistory(f\"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}\")\n",
        "    if type(input_text) == tuple:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Describe the image using as much detail as possible, is it a painting, a photograph, what colors are predominant, what is the image about?\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n",
        "        \"\"\" + input_text\n",
        "\n",
        "    writehistory(f\"prompt_instructions: {prompt_instructions}\")\n",
        "    prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "\n",
        "    # Properly extract the response text\n",
        "    if outputs is not None and len(outputs[0][\"generated_text\"]) > 0:\n",
        "        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
        "        if match:\n",
        "            # Extract the text after \"ASSISTANT:\"\n",
        "            reply = match.group(1)\n",
        "        else:\n",
        "            reply = \"No response found.\"\n",
        "    else:\n",
        "        reply = \"No response generated.\"\n",
        "\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdk0zGzNvMcq"
      },
      "outputs": [],
      "source": [
        "def transcribe(model, audio):\n",
        "\n",
        "    # Check if the audio input is None or empty\n",
        "    if audio is None or audio == '':\n",
        "        return ('','',None)  # Return empty strings and None audio file\n",
        "\n",
        "    # language = 'en'\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "\n",
        "    return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmNmMBlivRFx"
      },
      "outputs": [],
      "source": [
        "def text_to_speech(text, file_path):\n",
        "    language = 'en'\n",
        "\n",
        "    audioobj = gTTS(text = text,\n",
        "                    lang = language,\n",
        "                    slow = False)\n",
        "\n",
        "    audioobj.save(file_path)\n",
        "\n",
        "    return file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5mv8LErwPUE",
        "outputId": "fea9fce1-97d7-433e-ba9a-34823bb056da"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "ub9yV5DkvUS6",
        "outputId": "065d63d6-4ef0-4d87-fcd5-e81260593a38"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# A function to handle audio and image inputs\n",
        "def process_inputs(audio_path, image_path):\n",
        "    # Process the audio file (assuming this is handled by a function called 'transcribe')\n",
        "    speech_to_text_output = transcribe(model, audio_path)\n",
        "\n",
        "    # Handle the image input\n",
        "    if image_path:\n",
        "        chatgpt_output = img2txt(speech_to_text_output, image_path)\n",
        "    else:\n",
        "        chatgpt_output = \"No image provided.\"\n",
        "\n",
        "    # Assuming 'transcribe' also returns the path to a processed audio file\n",
        "    processed_audio_path = text_to_speech(chatgpt_output, \"Temp3.mp3\")  # Replace with actual path if different\n",
        "\n",
        "    return speech_to_text_output, chatgpt_output, processed_audio_path\n",
        "\n",
        "# Create the interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_inputs,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
        "        gr.Image(type=\"filepath\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Speech to Text\"),\n",
        "        gr.Textbox(label=\"ChatGPT Output\"),\n",
        "        gr.Audio(\"Temp.mp3\")\n",
        "    ],\n",
        "    title=\"Learn OpenAI Whisper: Image processing with Whisper and Llava\",\n",
        "    description=\"Upload an image and interact via voice input and audio response.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivAWLZGRwDGz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39edb5ab95f74f3e961bce9c8d763342": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8d2dedc3792421db386d1cb5d603374",
              "IPY_MODEL_ffe339cce05546c384487b232b8137bd",
              "IPY_MODEL_f9bb704cbea64f95a2d4808975cc71ee"
            ],
            "layout": "IPY_MODEL_faa3cf9476d945ddb04a9c149825831b"
          }
        },
        "607f0633ebcf40e4aa0b3acbd3b022a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a596e5e4c2514864a76d9f6540b893a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32d3736f87f495f944b37d35c8884ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cba69ea4f8604a939f71aab783cc989d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8d2dedc3792421db386d1cb5d603374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_607f0633ebcf40e4aa0b3acbd3b022a8",
            "placeholder": "​",
            "style": "IPY_MODEL_c32d3736f87f495f944b37d35c8884ed",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "eafb55cca4c94e899b05677e3b3b5b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9bb704cbea64f95a2d4808975cc71ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a596e5e4c2514864a76d9f6540b893a2",
            "placeholder": "​",
            "style": "IPY_MODEL_cba69ea4f8604a939f71aab783cc989d",
            "value": " 3/3 [00:06&lt;00:00,  2.05s/it]"
          }
        },
        "faa3cf9476d945ddb04a9c149825831b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc807b69b0aa47d9926eaa1adff16897": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe339cce05546c384487b232b8137bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc807b69b0aa47d9926eaa1adff16897",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eafb55cca4c94e899b05677e3b3b5b29",
            "value": 3
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
