
from chains import Chain
import shutil, os, asyncio
import streamlit as st
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage

# def ask_question(chain, query):
#     result = chain.invoke(query)
#     return result["result"]

os.makedirs('data', exist_ok=True)


st.set_page_config(page_title='RAG - Q&A Bot', page_icon='ðŸ”—')
st.title("Chat with PDF, Excel or CSV - ðŸ“šðŸš€")

openai_api_key = st.sidebar.text_input("Enter your OpenAI GPT-4 API key", placeholder='OpenAI GPT-4 API Key...')
file = st.sidebar.file_uploader("Upload File")

if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []
                
#write history
for message in st.session_state['chat_history']:
    if isinstance(message, HumanMessage):
        with st.chat_message('user'):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message('assistant'):
            st.markdown(message.content)
    
if openai_api_key and file:
    try:
        shutil.rmtree('data')
    except:
        pass
    if 'memory' not in st.session_state:
        st.session_state['memory'] = ConversationBufferWindowMemory(
                                            memory_key='history',
                                            input_key='question',
                                            k=1,
                                            return_messages=True
                                            )
    # st.session_state['file'] = file
    chain = Chain(file, openai_api_key, st.session_state['memory'])
    rag_chain = chain.create_chain()
    if rag_chain is None:
        st.sidebar.error('Error: Upload only PDF, Excel or CSV file', icon="ðŸš¨")   
    else:
        with st.chat_message("assistant"):
            st.write("Hello ! Ask me about your uploaded files ðŸ¤—")
            
        prompt = st.chat_input("Ask about your PDF, CSV or Excel data ðŸ§®")
        if prompt:
            st.session_state['chat_history'].append(HumanMessage(prompt))
            with st.chat_message("user"):
                st.markdown(prompt)
                
            with st.chat_message("assistant"):
                ai_response = st.write(rag_chain.invoke(prompt)['result'])
                # rag_chain.stream({'query':prompt}))
            # st.session_state['chat_history'].append(AIMessage(ai_response))
    