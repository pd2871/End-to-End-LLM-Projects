{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f8e6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import csv\n",
    "import os\n",
    "import sys, json_repair\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c826bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(output_file, data, mode='a'):\n",
    "    with open(output_file, mode, newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9431965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt(txt_path, data):\n",
    "    with open(txt_path, 'a') as txf:\n",
    "        txf.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21c1dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "\n",
    "model_name = 'gpt-4-turbo-2024-04-09'  \n",
    "title = 'Consumer Survey'  \n",
    "audience = 'Survey to people about their dog' \n",
    "output_file = 'dallasRun-test.csv'\n",
    "\n",
    "questions = [\n",
    "\"How old is your dog (in # of years)?\",\n",
    "\"What is your dog's primary breed?\",\n",
    "\"Approximately how much does your dog weigh (in pounds)?\",\n",
    "\"What is your dog's name?\",\n",
    "\"What was the inspiration for that name?\",\n",
    "\"Did you adopt your dog? (yes/no)\",\n",
    "\"How old was your dog when you got him/her? Provide estimated number of months old.\",\n",
    "\"How did you feel the first time you met your dog?\",\n",
    "\"How has your dog changed your outlook on life?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f82ba8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is not needed, it is there to know how function calling works.\n",
    "\n",
    "# To learn more about function calling, check this out https://www.datacamp.com/tutorial/open-ai-function-calling-tutorial\n",
    "#Answer is the class which contains response given by each user, it also contain traits to know which person answered the questions.\n",
    "#llm is inteligent enough to give unique response each time based on the given system and user prompt, we should not worry about it.\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Human-like set of answers told by each person while being surveyed. \n",
    "    Each person should have their own unique set of answers that should be different from other person's set of answers.\"\"\"\n",
    "\n",
    "    answer1: str = Field(description=\"This is the human like answer to Question1.\")\n",
    "    answer2: str = Field(description=\"This is the human like answer to Question2.\")\n",
    "    answer3: str = Field(description=\"This is the human like answer to Question3.\")\n",
    "    answer4: str = Field(description=\"This is the human like answer to Question4.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#list of answers, the formatted list that the llm model will generate which contains dictionaries of Answer class\n",
    "#llm model will make sure that each dictionary in the list is unique from each other as it will treat the list like list of human responses.\n",
    "class Answers(BaseModel):\n",
    "    \"\"\"Human-like survey answers to tell user.\"\"\"\n",
    "\n",
    "    answer: List[Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848abc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is standard pattern of list of functions used for function calling.\n",
    "# To know the pattern, you can print out openai_functions variable in previous New.ipynb file\n",
    "# This function is responsible to give the output list of answers of each human that we want, based on the parameters provided.\n",
    "# To learn more about function calling, check this out https://www.datacamp.com/tutorial/open-ai-function-calling-tutorial\n",
    "def create_openai_functions(questions: List):\n",
    "    fn  = [{'name': 'Answers',\n",
    "            'description': 'Human-like survey answers to tell user.',\n",
    "            'parameters': {'type': 'object',\n",
    "            'properties': {'answer': {'type': 'array',\n",
    "                'items': {'description': \"Human-like set of answers told by each person while being surveyed. \\nEach person should have their own unique set of answers that should be different from other person's set of answers.\",\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                },\n",
    "                'required': []}}},\n",
    "            'required': ['answer']}}]\n",
    "    answers = fn[0]['parameters']['properties']['answer']['items']['properties']\n",
    "    required = fn[0]['parameters']['properties']['answer']['items']['required']\n",
    "    for ind, _ in enumerate(questions, 1):\n",
    "        answers[f'answer{ind}'] = {'description': f'This is the human like answer to Question{ind}.', 'type': 'string'}\n",
    "        required.append(f'answer{ind}')\n",
    "    return fn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48af2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample openai function example\n",
    "openai_functions = create_openai_functions(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a51ced18",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonKeyOutputFunctionsParser(key_name=\"answer\")  # to parse the output, it will return list of answers based on function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbfabebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bind openai_functions to llm for function calling\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=model_name,\n",
    "    max_tokens=4095\n",
    ").bind(functions=openai_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3db3017-5355-4f7f-957d-4aafa2be0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "\n",
    "Assume you are a group of Women, Age 45-60 when answering all of the following questions\n",
    "\n",
    "Directly answer each question in the first person, providing the requested information, without additional context unless itâ€™s directly relevant. Your responses should embody the individual with the traits provided, showcasing a natural human variability in answering.\n",
    "\n",
    "- Be sure that you answer every single question. Do not miss answering any question.\n",
    "\n",
    "- When addressing gender and the traits are not clear, choose 'male' or 'female' based on the context provided by the traits. If traits suggest a higher likelihood for one gender over the other, use that as a basis for your choice.\n",
    "\n",
    "- Maintain consciousness and avoid AI-like language. The responses should feel personal, as if they are derived from the individual's own experiences and knowledge.\n",
    "\n",
    "- Introduce variety in your responses to mirror the natural differences in human behavior and decision-making. No two humans are exactly alike, so your answers should reflect a range of possible reactions and thoughts.\n",
    "\n",
    "- Humans do have things in common though so make sure that the results reflect that and have distributions like an expected survey would have.\n",
    "\n",
    "- Use the context from previous answers for each person to inform your responses to other questions for that individual. \n",
    "\n",
    "- If a question is asked for which the answer is not directly provided by the traits, make an educated guess based on the available information and what is known about typical behaviors or preferences of people with similar traits.\n",
    "\n",
    "- Do not include any other questions and answers in your response. Just give the answer to the question you are being asked.\n",
    "\n",
    "-The aim is to craft responses that are individually tailored, exhibit the complexity and subtlety of human thought, and are coherent throughout the survey, as if the answers are coming from one person with a consistent set of traits and experiences.\n",
    "\n",
    "-Ensure the answers are short and match the output asked for in the questions.\n",
    "If the output asks for a numerical output just give the number. For example if you \n",
    "are asked how old are you just give the number and don't have any text before or after that unless prompted to do so.\n",
    "\n",
    "If the output asks for a monetary output, give just the dollar amount and no text before or after it unless prompted to do so. So for example if the answer is \n",
    "$40 just give $40.\n",
    "\n",
    "If the output asks for a decimal output, give just the value and no text before or after it unless prompted to do so. So for example if the answer is \n",
    "40.5 just give 40.5.\n",
    "\n",
    "If you are asked a question like where do you live? Be sure to just answer the place that you live. No text before or after it is needed unless prompted to do so.\n",
    "\n",
    "If the answer is asking for a percentage output just give the value. So if the answer is 45% just give 45%. Do not give any text before or after that unless prompted to do so.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffcbdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_message), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29be918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe9869fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser(response):\n",
    "    try:\n",
    "        return eval(response.additional_kwargs['function_call']['arguments'])['answer']\n",
    "    except Exception as e:\n",
    "        # print(\"Error in Eval\\n\")\n",
    "        # print(e)\n",
    "        # print('----------------------------------------------------------------------')\n",
    "        try:\n",
    "            return json_repair.loads(response.additional_kwargs['function_call']['arguments'])['answer']\n",
    "        except Exception as e:\n",
    "            # print(\"Error in Json loads\")\n",
    "            # print(e)\n",
    "            # print('----------------------------------------------------------------------')\n",
    "            # write_txt(txt_file, \"\\nFailed-eval-json-loads---------------------------------------\\n\\n\\n\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44adcea6-f11a-480e-bcc0-fcd150e3c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_token(response):\n",
    "    try:\n",
    "        return int(response.response_metadata['token_usage']['completion_tokens'])\n",
    "    except Exception as e:\n",
    "        # print(\"Error in Eval\\n\")\n",
    "        # print(e)\n",
    "        # print('----------------------------------------------------------------------')\n",
    "        try:\n",
    "            return int(response.response_metadata['token_usage']['completion_tokens'])\n",
    "        except Exception as e:\n",
    "            # print(\"Error in Json loads\")\n",
    "            # print(e)\n",
    "            # print('----------------------------------------------------------------------')\n",
    "            # write_txt(txt_file, \"\\nFailed-eval-json-loads---------------------------------------\\n\\n\\n\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ca1b444-0399-40a2-9bb1-6257de498b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'arguments': '{\"answer\":[{\"answer1\":\"7\",\"answer2\":\"Labrador Retriever\",\"answer3\":\"75\",\"answer4\":\"Buddy\",\"answer5\":\"He just looked like a Buddy to us; it fit perfectly.\",\"answer6\":\"yes\",\"answer7\":\"8\",\"answer8\":\"I felt an instant connection and immense joy.\",\"answer9\":\"He\\'s taught me the value of unconditional love and patience.\"}]}', 'name': 'Answers'}} response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 971, 'total_tokens': 1063}, 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_76f018034d', 'finish_reason': 'function_call', 'logprobs': None} id='run-e6ad44aa-3c72-4b97-a0a6-3fa5b7d6ee76-0'\n",
      "Total tokens: 92\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"answer\":[{\"answer1\":\"7\",\"answer2\":\"Labrador Retriever\",\"answer3\":\"75\",\"answer4\":\"Buddy\",\"answer5\":\"He just looked like a Buddy to us; it fit perfectly.\",\"answer6\":\"yes\",\"answer7\":\"8\",\"answer8\":\"I felt an instant connection and immense joy.\",\"answer9\":\"He\\'s taught me the value of unconditional love and patience.\"}]}', 'name': 'Answers'}} response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 971, 'total_tokens': 1063}, 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_76f018034d', 'finish_reason': 'function_call', 'logprobs': None} id='run-e6ad44aa-3c72-4b97-a0a6-3fa5b7d6ee76-0'\n",
      "\n",
      "\n",
      "Maximum responses per batch based on token usage: 42\n",
      "Processing batch of 42 responses for trait: Women Aged 45-60 and own 1 dog\n"
     ]
    }
   ],
   "source": [
    "traits_and_counts = [\n",
    "    ('Women Aged 45-60 and own 1 dog', 100),\n",
    "]\n",
    "\n",
    "\n",
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# def count_tokens(text):\n",
    "#     # Load the tokenizer\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "#     # Tokenize the text and count the tokens\n",
    "#     tokens = tokenizer.encode(text)\n",
    "#     return len(tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_message = ''\n",
    "test_message = f\"Generate survey answers from 1 person.\\nAll of the surveyed people are {audience}\\n\"\n",
    "test_message += f'The title of the survey: {title}\\n'\n",
    "test_message += f'1 of the surveyed people have this trait: ' + traits_and_counts[0][0] + ''\n",
    "test_message += 'Please answer all questions below:\\n'\n",
    "\n",
    "for question in questions:\n",
    "    test_message += f'{question}\\n'\n",
    "\n",
    "TestRes = chain.invoke({\"input\": test_message})\n",
    "responseTest = output_parser(TestRes)\n",
    "total_tokens = get_output_token(TestRes)\n",
    "\n",
    "# Concatenate all answers into a single string\n",
    "# all_answers_text = \" \".join(answer for response in responseTest for answer in response.values())\n",
    "\n",
    "# # Count tokens in the concatenated text\n",
    "# total_tokens = count_tokens(all_answers_text)\n",
    "print(\"Total tokens:\", total_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# You can now use this token count to calculate how many responses fit within a specific token budget\n",
    "tokens_per_batch = 3900\n",
    "max_responses_per_batch = tokens_per_batch // int(total_tokens)\n",
    "print(f\"Maximum responses per batch based on token usage: {max_responses_per_batch}\")\n",
    "\n",
    "\n",
    "counts = 0\n",
    "total_processed_row = 0\n",
    "processed_this_batch=0\n",
    "person_number=0\n",
    "output_file = 'dallasRunTest.csv'\n",
    "\n",
    "\n",
    "\n",
    "traits_dict = dict(traits_and_counts)\n",
    "\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    headers = ['Traits', 'Person Number'] + questions\n",
    "    write_csv(output_file, headers, mode='w')\n",
    "\n",
    "\n",
    "for traits, counts in traits_dict.items():\n",
    "    if counts <= 0:\n",
    "        continue\n",
    "\n",
    "    while counts > 0:\n",
    "        current_batch_size = min(max_responses_per_batch, counts)\n",
    "        print(f\"Processing batch of {current_batch_size} responses for trait: {traits}\")\n",
    "\n",
    "        try:\n",
    "            input_message = f\"Generate survey answers from {current_batch_size} people.\\nAll of the surveyed people are {audience}\\n\"\n",
    "            input_message += f'The title of the survey: {title}\\n'\n",
    "            input_message += f'{current_batch_size} of the surveyed people have this trait: {traits}\\n'\n",
    "            input_message += 'Please answer all questions below:\\n'\n",
    "            for question in questions:\n",
    "                input_message += f'{question}\\n'\n",
    "\n",
    "            res = chain.invoke({\"input\": input_message})\n",
    "            responses = output_parser(res)\n",
    "            \n",
    "            if responses is None or not responses:\n",
    "                print(\"No valid responses or error occurred\")\n",
    "                continue\n",
    "\n",
    "            processed_this_batch = len(responses)\n",
    "            counts -= processed_this_batch  # Decrement counts by the number of processed responses\n",
    "            \n",
    "            for response in responses:\n",
    "                person_number += 1\n",
    "                response_data = [traits, person_number]\n",
    "                # Using an empty string as the default value for missing answers\n",
    "                response_data.extend(response.get(f'answer{i+1}', '') for i in range(len(questions)))\n",
    "                write_csv(output_file, response_data)\n",
    "\n",
    "            total_processed_row += processed_this_batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'OpenAI Error:', e)\n",
    "            continue\n",
    "\n",
    "        print(f\"Batch complete. {len(responses)} responses processed.\")\n",
    "        print(f'Remaining counts for {traits}: {counts}')\n",
    "        print(\"Total responses given by model:\", processed_this_batch)\n",
    "        print(\"Total processed responses:\", total_processed_row)\n",
    "\n",
    "        if counts <= 0:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb07f10-ad12-4f90-82d0-886603d8721e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
